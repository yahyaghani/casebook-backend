{'ids': [['REAC_T:_SYNERGIZING_REASONING_AND_ACTING_IN_LANGUAGE_MODELS_https://arxiv.org/pdf/2210.03629_chunk_9', 'REAC_T:_SYNERGIZING_REASONING_AND_ACTING_IN_LANGUAGE_MODELS_https://arxiv.org/pdf/2210.03629_chunk_1', 'REAC_T:_SYNERGIZING_REASONING_AND_ACTING_IN_LANGUAGE_MODELS_https://arxiv.org/pdf/2210.03629_chunk_8', 'REAC_T:_SYNERGIZING_REASONING_AND_ACTING_IN_LANGUAGE_MODELS_https://arxiv.org/pdf/2210.03629_chunk_7', 'REAC_T:_SYNERGIZING_REASONING_AND_ACTING_IN_LANGUAGE_MODELS_https://arxiv.org/pdf/2210.03629_chunk_6']], 'distances': [[0.3767392568424806, 0.40637102147795795, 0.4167675176055388, 0.4384281840693862, 0.44265371565998557]], 'metadatas': [[{'chunk': 9, 'title': 'REAC T: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS', 'total_chunks': 19, 'url': 'https://arxiv.org/pdf/2210.03629'}, {'chunk': 1, 'title': 'REAC T: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS', 'total_chunks': 19, 'url': 'https://arxiv.org/pdf/2210.03629'}, {'chunk': 8, 'title': 'REAC T: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS', 'total_chunks': 19, 'url': 'https://arxiv.org/pdf/2210.03629'}, {'chunk': 7, 'title': 'REAC T: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS', 'total_chunks': 19, 'url': 'https://arxiv.org/pdf/2210.03629'}, {'chunk': 6, 'title': 'REAC T: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS', 'total_chunks': 19, 'url': 'https://arxiv.org/pdf/2210.03629'}]], 'embeddings': None, 'documents': [['Pre-trainedlanguagemodelsforinteractivedecision-making,2022. URLhttps: //arxiv.org/abs/2202.01771. 11 PublishedasaconferencepaperatICLR2023 AleksandrRomanovichLuria. Lsvygotskyandtheproblemoflocalizationoffunctions. Neuropsy- chologia,3(4):387–392,1965. AmanMadaanandAmirYazdanbakhsh. Textandpatterns: Foreffectivechainofthought,ittakes twototango,2022. URLhttps://arxiv.org/abs/2209.07686. Vincent Micheli and François Fleuret. Language models are few-shot butlers. arXiv preprint arXiv:2104.07972,2021. ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,ChristinaKim,Christopher Hesse,ShantanuJain,VineetKosaraju,WilliamSaunders,XuJiang,KarlCobbe,TynaEloundou, GretchenKrueger,KevinButton,MatthewKnight,BenjaminChess,andJohnSchulman. Webgpt: Browser-assisted question-answering with human feedback, 2021. URL https://arxiv. org/abs/2112.09332. MaxwellNye,AndersJohanAndreassen,GuyGur-Ari,HenrykMichalewski,JacobAustin,David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models,2021. URLhttps://arxiv.org/abs/2112.00114. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, GabrielBarth-Maron, MaiGimenez, YurySulsky, JackieKay, JostTobiasSpringenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, OriolVinyals,MahyarBordbar,andNandodeFreitas. Ageneralistagent,2022. URLhttps: //arxiv.org/abs/2205.06175. MohitShridhar,JesseThomason,DanielGordon,YonatanBisk,WinsonHan,RoozbehMottaghi, LukeZettlemoyer,andDieterFox. Alfred: Abenchmarkforinterpretinggroundedinstructions foreverydaytasks. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern recognition,pp.10740–10749,2020a. MohitShridhar,XingdiYuan,Marc-AlexandreCôté,YonatanBisk,AdamTrischler,andMatthew Hausknecht. Alfworld: Aligningtextandembodiedenvironmentsforinteractivelearning. arXiv preprintarXiv:2010.03768,2020b. KurtShuster,MojtabaKomeili,LeonardAdolphs,StephenRoller,ArthurSzlam,andJasonWeston. Languagemodelsthatseekforknowledge: Modularsearch&generationfordialogueandprompt completion. arXivpreprintarXiv:2203.13224,2022a. KurtShuster,JingXu,MojtabaKomeili,DaJu,EricMichaelSmith,StephenRoller,MeganUng, MoyaChen,KushalArora,JoshuaLane,MortezaBehrooz,WilliamNgan,SpencerPoff,Naman Goyal, Arthur Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage, 2022b. URL https://arxiv.org/abs/2208.03188. JamesThorne,AndreasVlachos,ChristosChristodoulopoulos,andArpitMittal. Fever: alarge-scale datasetforfactextractionandverification. arXivpreprintarXiv:1803.05355,2018. LevSVygotsky. Thinkingandspeech. ThecollectedworksofLSVygotsky,1:39–285,1987. XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,AakankshaChowdh- ery,andDennyZhou. Self-consistencyimproveschainofthoughtreasoninginlanguagemodels, 2022a. URLhttps://arxiv.org/abs/2203.11171. XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,andDennyZhou.Rationale-augmented ensemblesinlanguagemodels. arXivpreprintarXiv:2207.00747,2022b. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chainofthoughtpromptingelicitsreasoninginlargelanguagemodels. arXivpreprint arXiv:2201.11903,2022. ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamWCohen,RuslanSalakhutdinov, andChristopherDManning. Hotpotqa: Adatasetfordiverse, explainablemulti-hopquestion answering. arXivpreprintarXiv:1809.09600,2018. 12 PublishedasaconferencepaperatICLR2023 ShunyuYao,RohanRao,MatthewHausknecht,andKarthikNarasimhan. KeepCALMandexplore: Languagemodelsforactiongenerationintext-basedgames.InProceedingsofthe2020Conference onEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pp.8736–8754,Online,Novem- ber2020.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.emnlp-main.704. URLhttps://aclanthology.org/2020.emnlp-main.704. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. arXiv preprint arXiv:2207.01206, 2022. EricZelikman,YuhuaiWu,JesseMu,andNoahD.Goodman. Star: Bootstrappingreasoningwith reasoning,2022. URLhttps://arxiv.org/abs/2203.14465. DennyZhou,NathanaelSchärli,LeHou,JasonWei,NathanScales,XuezhiWang,DaleSchuurmans, OlivierBousquet,QuocLe,andEdChi. Least-to-mostpromptingenablescomplexreasoningin largelanguagemodels,2022. URLhttps://arxiv.org/abs/2205.10625. YunchangZhu,LiangPang,YanyanLan,HuaweiShen,andXueqiCheng. Adaptiveinformation seekingforopen-domainquestionanswering. arXivpreprintarXiv:2109.06747,2021. 13 PublishedasaconferencepaperatICLR2023 A ADDITIONAL RESULTS A.1 GPT-3EXPERIMENTS PaLM-540B GPT-3 HotpotQA(exactmatch) 29.4 30.8 ALFWorld(successrate%) 70.9 78.4 Table5: ReActpromptingresultsusingPaLM-540Bvs.GPT-3(text-davinci-002,greedydecoding). OnHotpotQA,werandomlysampleasubsetof500validationquestions. OnALFWorld,weuseall 134unseenvalidationtaskinstances,andusethebestpromptsetaccordingtoPaLM-540B. WerunadditionalGPT-3(Brownetal.,2020)experimentstoconfirmReActpromptingperformance isgeneralacrossdifferentlargelanguagemodels. AsshowninTable5,GPT-3(text-davinci-002, greedy decoding) consistently outperforms PaLM-540B on HotpotQA and ALFWorld, possibly becauseitisfinetunedwithhumaninstructionfollowing. ThisindicatesReActpromptingiseffective across different large language models on different tasks. The code for these experiments are at https://react-lm.github.io/. A.2 REACT OBTAINSUP-TO-DATEKNOWLEDGEONHOTPOTQA (cid:11)(cid:20)(cid:12)(cid:3)(cid:43)(cid:82)(cid:87)(cid:86)(cid:83)(cid:82)(cid:87)(cid:3)(cid:52)(cid:36) (cid:11)(cid:20)(cid:71)(cid:12)(cid:3)(cid:53)(cid:72)(cid:36)(cid:70)(cid:87)(cid:3)(cid:11)(cid:53)(cid:72)(cid:68)(cid:86)(cid:82)(cid:81)(cid:3)(cid:14)(cid:3)(cid:36)(cid:70)(cid:87)(cid:12) (cid:52)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29)(cid:3)(cid:43)(cid:82)(cid:90)(cid:3)(cid:80)(cid:68)(cid:81)(cid:92)(cid:3)(cid:85)(cid:82)(cid:82)(cid:80)(cid:86)(cid:3)(cid:68)(cid:85)(cid:72)(cid:3)(cid:76)(cid:81)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:75)(cid:82)(cid:87)(cid:72)(cid:79)(cid:3)(cid:87)(cid:75)(cid:68)(cid:87)(cid:3)(cid:76)(cid:86)(cid:3)(cid:75)(cid:82)(cid:80)(cid:72)(cid:3)(cid:87)(cid:3)(cid:82) (cid:55)(cid:75)(cid:82)(cid:88)(cid:74)(cid:75)(cid:87)(cid:3)(cid:20)(cid:29)(cid:3)(cid:44)(cid:3)(cid:81)(cid:72)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:86)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:38)(cid:76)(cid:85)(cid:84)(cid:88)(cid:72)(cid:3)(cid:71)(cid:88)(cid:3)(cid:54)(cid:82)(cid:79)(cid:72)(cid:76)(cid:79)(cid:3)(cid:86)(cid:75)(cid:82)(cid:90)(cid:3)(cid:48)(cid:92)(cid:86)(cid:87)(cid:72)(cid:85)(cid:72)(cid:15) (cid:3) (cid:87)(cid:75)(cid:72)(cid:3)(cid:38)(cid:76)(cid:85)(cid:84)(cid:88)(cid:72)(cid:3)(cid:71)(cid:88)(cid:3)(cid:54)(cid:82)(cid:79)(cid:72)(cid:76)(cid:79)(cid:3)(cid:86)(cid:75)(cid:82)(cid:90)(cid:3)(cid:48)(cid:92)(cid:86)(cid:87)(cid:72)(cid:85)(cid:72)(cid:34) (cid:73)(cid:76)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:75)(cid:82)(cid:87)(cid:72)(cid:79)(cid:3)(cid:76)(cid:87)(cid:3)(cid:76)(cid:86)(cid:3)(cid:76)(cid:81)(cid:15)(cid:3)(cid:87)(cid:75)(cid:72)(cid:81)(cid:3)(cid:73)(cid:76)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:81)(cid:88)(cid:80)(cid:69)(cid:72)(cid:85)(cid:3)(cid:82)(cid:73)(cid:3)(cid:85)(cid:82)(cid:82)(cid:80)(cid:86)(cid:3)(cid:76)(cid:81) (cid:3) (cid:43)(cid:82)(cid:87)(cid:83)(cid:82)(cid:87)(cid:52)(cid:36)(cid:3)(cid:79)(cid:68)(cid:69)(cid:72)(cid:79)(cid:29)(cid:3)(cid:21)(cid:15)(cid:25)(cid:25)(cid:23) (cid:50)(cid:88)(cid:87)(cid:71)(cid:68)(cid:87)(cid:72)(cid:71)(cid:3) (cid:87) (cid:36)(cid:75) (cid:70)(cid:72) (cid:87)(cid:3) (cid:3)(cid:75) (cid:20)(cid:82) (cid:29)(cid:3)(cid:87)(cid:72) (cid:54)(cid:79) (cid:72)(cid:17) (cid:68)(cid:85)(cid:70)(cid:75)(cid:62)(cid:38)(cid:76)(cid:85)(cid:84)(cid:88)(cid:72)(cid:3)(cid:71)(cid:88)(cid:3)(cid:54)(cid:82)(cid:79)(cid:72)(cid:76)(cid:79)(cid:3)(cid:86)(cid:75)(cid:82)(cid:90)(cid:3)(cid:48)(cid:92)(cid:86)(cid:87)(cid:72)(cid:85)(cid:72)(cid:64)', 'PublishedasaconferencepaperatICLR2023 REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS ShunyuYao∗*,1,JeffreyZhao2,DianYu2,NanDu2,IzhakShafran2,KarthikNarasimhan1,YuanCao2 1DepartmentofComputerScience,PrincetonUniversity 2GoogleResearch,Brainteam 1{shunyuy,karthikn}@princeton.edu 2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com ABSTRACT Whilelargelanguagemodels(LLMs)havedemonstratedimpressiveperformance across tasks in language understanding and interactive decision making, their abilitiesforreasoning(e.g. chain-of-thoughtprompting)andacting(e.g. action plangeneration)haveprimarilybeenstudiedasseparatetopics. Inthispaper,we exploretheuseofLLMstogeneratebothreasoningtracesandtask-specificactions inaninterleavedmanner,allowingforgreatersynergybetweenthetwo: reasoning traces help the model induce, track, and update action plans as well as handle exceptions,whileactionsallowittointerfacewithandgatheradditionalinformation fromexternalsourcessuchasknowledgebasesorenvironments. Weapplyour approach,namedReAct,toadiversesetoflanguageanddecisionmakingtasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering(HotpotQA)andfactverification(Fever),ReActovercomesprevalent issues of hallucination and error propagation in chain-of-thought reasoning by interactingwithasimpleWikipediaAPI,andgeneratinghuman-liketask-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop),ReActoutperformsimitationandreinforcementlearningmethodsby anabsolutesuccessrateof34%and10%respectively,whilebeingpromptedwith onlyoneortwoin-contextexamples. 1 INTRODUCTION Auniquefeatureofhumanintelligenceistheabilitytoseamlesslycombinetask-orientedactionswith verbalreasoning(orinnerspeech, Alderson-Day&Fernyhough,2015),whichhasbeentheorizedto playanimportantroleinhumancognitionforenablingself-regulationorstrategization(Vygotsky, 1987;Luria,1965;Fernyhough,2010)andmaintainingaworkingmemory(Baddeley,1992). Con- sidertheexampleofcookingupadishinthekitchen. Betweenanytwospecificactions,wemay reasoninlanguageinordertotrackprogress(“nowthateverythingiscut,Ishouldheatupthepotof water”),tohandleexceptionsoradjusttheplanaccordingtothesituation(“Idon’thavesalt,solet meusesoysauceandpepperinstead”),andtorealizewhenexternalinformationisneeded(“howdo Ipreparedough? LetmesearchontheInternet”). Wemayalsoact(openacookbooktoreadthe recipe,openthefridge,checkingredients)tosupportthereasoningandtoanswerquestions(“What dishcanImakerightnow?”). Thistightsynergybetween“acting”and“reasoning”allowshumans tolearnnewtasksquicklyandperformrobustdecisionmakingorreasoning,evenunderpreviously unseencircumstancesorfacinginformationuncertainties. Recentresultshavehintedatthepossibilityofcombiningverbalreasoningwithinteractivedecision makinginautonomoussystems. Ononehand,properlypromptedlargelanguagemodels(LLMs) have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive ∗WorkduringGoogleinternship.Projetpagewithcode:https://react-lm.github.io/. 1 3202 raM 01 ]LC.sc[ 3v92630.0122:viXra PublishedasaconferencepaperatICLR2023 (cid:11)(cid:20)(cid:12)(cid:3)(cid:43)(cid:82)(cid:87)(cid:86)(cid:83)(cid:82)(cid:87)(cid:3)(cid:52)(cid:36) (cid:11)(cid:20)(cid:71)(cid:12)(cid:3)(cid:53)(cid:72)(cid:36)(cid:70)(cid:87)(cid:3)(cid:11)(cid:53)(cid:72)(cid:68)(cid:86)(cid:82)(cid:81)(cid:3)(cid:14)(cid:3)(cid:36)(cid:70)(cid:87)(cid:12) (cid:52)(cid:88)(cid:72)(cid:86)(cid:87)(cid:76)(cid:82)(cid:81)(cid:29)(cid:3)(cid:36)(cid:86)(cid:76)(cid:71)(cid:72)(cid:3)(cid:73)(cid:85)(cid:82)(cid:80)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:36)(cid:83)(cid:83)(cid:79)(cid:72)(cid:3)(cid:53)(cid:72)(cid:80)(cid:82)(cid:87)(cid:72)(cid:15)(cid:3)(cid:90)(cid:75)(cid:68)(cid:87)(cid:3)(cid:82)(cid:87)(cid:75)(cid:72)(cid:85)(cid:3)(cid:71)(cid:72)(cid:89)(cid:76)(cid:70)(cid:3)(cid:72) (cid:55)(cid:75)(cid:82)(cid:88)(cid:74)(cid:75)(cid:87)(cid:3)(cid:20)(cid:29)(cid:3)(cid:44)(cid:3)(cid:81)(cid:72)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:86)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:3)(cid:36)(cid:83)(cid:83)(cid:79)(cid:72)(cid:3)(cid:53)(cid:72)(cid:80)(cid:82)(cid:87)(cid:72)(cid:3)(cid:68)(cid:81)(cid:71)(cid:3)(cid:73)(cid:76)(cid:81)(cid:71)(cid:3)(cid:87)(cid:75)(cid:72) (cid:3) (cid:70)(cid:68)(cid:81)(cid:3)(cid:70)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:3)(cid:87)(cid:75)(cid:72)(cid:3)(cid:83)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:36)(cid:83)(cid:83)(cid:79)(cid:72)(cid:3)(cid:53)(cid:72)(cid:80)(cid:82)(cid:87)(cid:72)(cid:3)(cid:90)(cid:68)(cid:86)(cid:3)(cid:82)(cid:85)(cid:76)(cid:74)(cid:76)(cid:81)(cid:68)(cid:79)(cid:79)(cid:3)(cid:92) (cid:83)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:76)(cid:87)(cid:3)(cid:90)(cid:68)(cid:86)(cid:3)(cid:82)(cid:85)(cid:76)(cid:74)(cid:76)(cid:81)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3)(cid:71)(cid:72)(cid:86)(cid:76)(cid:74)(cid:81)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:68)(cid:70)(cid:87)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:17) (cid:36)(cid:70)(cid:87)(cid:3)(cid:20)(cid:29)(cid:3)(cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:62)(cid:36)(cid:83)(cid:83)(cid:79)(cid:72)(cid:3)(cid:53)(cid:72)(cid:80)(cid:82)(cid:87)(cid:72)(cid:64) (cid:71)(cid:72)(cid:86)(cid:76)(cid:74)(cid:81)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:76)(cid:81)(cid:87)(cid:72)(cid:85)(cid:68)(cid:70)(cid:87)(cid:3)(cid:90)(cid:76)(cid:87)(cid:75)(cid:34) (cid:50)(cid:69)(cid:86)(cid:3)(cid:20)(cid:29)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:36)(cid:83)(cid:83)(cid:79)(cid:72)(cid:3)(cid:53)(cid:72)(cid:80)(cid:82)(cid:87)(cid:72)(cid:3)(cid:76)(cid:86)(cid:3)(cid:68)(cid:3)(cid:85)(cid:72)(cid:80)(cid:82)(cid:87)(cid:72)(cid:3)(cid:70)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:3)(cid:76)(cid:81)(cid:87)(cid:85)(cid:82)(cid:71)(cid:88)(cid:70)(cid:72)(cid:71)(cid:3)(cid:76)(cid:81) (cid:3) (cid:50)(cid:70)(cid:87)(cid:82)(cid:69)(cid:72)(cid:85)(cid:3)(cid:21)(cid:19)(cid:19)(cid:24)(cid:3)(cid:69)(cid:92)(cid:3)(cid:36)(cid:83)(cid:83)(cid:79)(cid:72)(cid:3)(cid:170)(cid:3)(cid:82)(cid:85)(cid:76)(cid:74)(cid:76)(cid:81)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3)(cid:71)(cid:72)(cid:86)(cid:76)(cid:74)(cid:81)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:70)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79)(cid:3)(cid:87)(cid:75)(cid:72) (cid:3) (cid:11)(cid:20)(cid:68)(cid:12)(cid:3)(cid:54)(cid:87)(cid:68)(cid:81)(cid:71)(cid:68)(cid:85)(cid:71) (cid:11)(cid:20)(cid:70)(cid:12)(cid:3)(cid:36)(cid:70)(cid:87)(cid:16)(cid:50)(cid:81)(cid:79)(cid:92)(cid:3) (cid:41)(cid:85)(cid:82)(cid:81)(cid:87)(cid:3)(cid:53)(cid:82)(cid:90)(cid:3)(cid:80)(cid:72)(cid:71)(cid:76)(cid:68)(cid:3)(cid:70)(cid:72)(cid:81)(cid:87)(cid:72)(cid:85)(cid:3)(cid:83)(cid:85)(cid:82)(cid:74)(cid:85)(cid:68)(cid:80)(cid:3)(cid:170) (cid:36)(cid:81)(cid:86)(cid:90)(cid:72)(cid:85)(cid:29)(cid:3)(cid:76)(cid:51)(cid:82)(cid:71) (cid:36)(cid:70)(cid:87)(cid:3)(cid:20)(cid:29)(cid:3)(cid:54)(cid:72)(cid:68)(cid:85)(cid:70)(cid:75)(cid:62)(cid:36)(cid:83)(cid:83)(cid:79)(cid:72)(cid:3)(cid:53)(cid:72)(cid:80)(cid:82)(cid:87)(cid:72)(cid:64)(cid:3) (cid:55)(cid:75)(cid:82)(cid:88)(cid:74)(cid:75)(cid:87)(cid:3)(cid:21)(cid:29)(cid:3)(cid:36)(cid:83)(cid:83)(cid:79)(cid:72)(cid:3)(cid:53)(cid:72)(cid:80)(cid:82)(cid:87)(cid:72)(cid:3)(cid:90)(cid:68)(cid:86)(cid:3)(cid:82)(cid:85)(cid:76)(cid:74)(cid:76)(cid:81)(cid:68)(cid:79)(cid:79)(cid:92)(cid:3)(cid:71)(cid:72)(cid:86)(cid:76)(cid:74)(cid:81)(cid:72)(cid:71)(cid:3)(cid:87)(cid:82)(cid:3)(cid:70)(cid:82)(cid:81)(cid:87)(cid:85)(cid:82)(cid:79) (cid:3) (cid:50)(cid:69)(cid:86)(cid:3)(cid:20)(cid:29)(cid:3)(cid:55)(cid:75)(cid:72)(cid:3)(cid:36)(cid:83)(cid:83)(cid:79)(cid:72)(cid:3)(cid:53)(cid:72)(cid:80)(cid:82)(cid:87)(cid:72)(cid:3)(cid:76)(cid:86)(cid:3)(cid:68)(cid:3)', 'thatInnerMonologuedoesnottrulycompriseofinnerthoughts—thisiselaboratedinSection4. We alsonotethatleveraginglanguageassemantically-richinputsintheprocessofinteractivedecision makinghasbeenshowntobesuccessfulunderothersettings(Abramsonetal.,2020;Karamcheti etal.,2021;Huangetal.,2022a;Lietal.,2022). Itisbecomingmoreevidentthatwiththehelpof LLMs,languageasafundamentalcognitivemechanismwillplayacriticalroleininteractionand decisionmaking. Whatismore,progressinLLMshasalsoinspiredthedevelopmentofversatileand generalistagentslikeReedetal.(2022). 6 CONCLUSION WehaveproposedReAct–asimpleyeteffectivemethodforsynergizingreasoningandactingin largelanguagemodels. Throughadiversesetofexperimentsonmulti-hopquestion-answering,fact checking,andinteractivedecision-makingtasks,weshowthatReActleadstosuperiorperformance withinterpretabledecisiontraces. Despitethesimplicityofourmethod,complextaskswithlarge actionspacesrequiremoredemonstrationstolearnwell,whichunfortunatelycaneasilygobeyond the input length limit of in-context learning. We explore the fine-tuning approach on HotpotQA 6Humanfeedbackcanalsobeincorporatedinacomplementarymannerbutweleaveitforfuturework. 9 PublishedasaconferencepaperatICLR2023 withinitialpromisingresults,butlearningfrommorehigh-qualityhumanannotationswillbethe desiderata to further improve the performance. Scaling up ReAct with multi-task training and combiningitwithcomplementaryparadigmslikereinforcementlearningcouldresultinstronger agentsthatfurtherunlockthepotentialofLLMsformoreapplications. ACKNOWLEDGMENTS WethankthesupportandfeedbackofmanypeoplefromGoogleBrainteamandPrincetonNLP Group. This work was supported in part by the National Science Foundation under Grant No. 2107048. Anyopinions,findings,andconclusionsorrecommendationsexpressedinthismaterialare thoseoftheauthor(s)anddonotnecessarilyreflecttheviewsoftheNationalScienceFoundation. REPRODUCIBILITYSTATEMENT OurmainexperimentsaredoneonPaLM(Chowdheryetal.,2022),whichisnotanopenlyaccessible modelyet. Toincreasereproducibility,wehaveincludedallusedpromptsinAppendixC,additional experiments using GPT-3 (Brown et al., 2020) in Appendix A.1, and associated GPT-3 ReAct promptingcodeathttps://anonymous.4open.science/r/ReAct-2268/. ETHICSSTATEMENT ReAct prompts large language models to generate more human interpretable, diagnosable, and controllabletask-solvingtrajectoriesthanpreviousmethods. However,hookingupalargelanguage modelwithanactionspacetointeractwithexternalenvironments(e.g.theweb,physicalenviron- ments)haspotentialdangers,e.g. lookingupinappropriateorprivateinformation,ortakingharmful actions in an environment. Our experiments minimize such risks by limiting the interactions to specificwebsites(WikipediaorWebShop)thatarefreeofprivateinformation,withoutanydangerous actionsintheactionspacedesign(i.e.modelscannotreallybuyproductsonWebShoptheresearch benchmark,oreditWikipedia).Webelieveresearchersshouldbeawareofsuchrisksbeforedesigning moreextensiveexperimentsinthefuture. REFERENCES JoshAbramson,ArunAhuja,IainBarr,ArthurBrussee,FedericoCarnevale,MaryCassin,Rachita Chhaparia,StephenClark,BogdanDamoc,AndrewDudzik,PetkoGeorgiev,AureliaGuy,Tim Harley,FelixHill,AldenHung,ZacharyKenton,JessicaLandon,TimothyLillicrap,KoryMathew- son,SonˇaMokrá,AlistairMuldal,AdamSantoro,NikolaySavinov,VikrantVarma,GregWayne, DuncanWilliams,NathanielWong,ChenYan,andRuiZhu. Imitatinginteractiveintelligence, 2020. URLhttps://arxiv.org/abs/2012.05672. MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,Chelsea Finn,ChuyuanFu,KeerthanaGopalakrishnan,KarolHausman,AlexHerzog,DanielHo,Jasmine Hsu,JulianIbarz,BrianIchter,AlexIrpan,EricJang,RosarioJaureguiRuano,KyleJeffrey,Sally Jesmonth, Nikhil J Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Kuang-Huei Lee, SergeyLevine,YaoLu,LindaLuu,CarolinaParada,PeterPastor,JornellQuiambao,Kanishka Rao,JarekRettinghouse,DiegoReyes,PierreSermanet,NicolasSievers,ClaytonTan,Alexander Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan Yan, and AndyZeng. Doasican,notasisay: Groundinglanguageinroboticaffordances,2022. URL https://arxiv.org/abs/2204.01691. Ben Alderson-Day and Charles Fernyhough. Inner speech: development, cognitive functions, phenomenology,andneurobiology. Psychologicalbulletin,141(5):931,2015. AlanBaddeley. Workingmemory. Science,255(5044):556–559,1992. TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal, ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020. 10 PublishedasaconferencepaperatICLR2023 Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scalinglanguagemodelingwithpathways. arXivpreprintarXiv:2204.02311,2022. AntoniaCreswellandMurrayShanahan. Faithfulreasoningusinglargelanguagemodels,2022. URL https://arxiv.org/abs/2208.14271. Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large languagemodelsforinterpretablelogicalreasoning,2022. URLhttps://arxiv.org/abs/ 2205.09712. AngelaFan,YacineJernite,EthanPerez,DavidGrangier,JasonWeston,andMichaelAuli. ELI5: Longformquestionanswering. InProceedingsofthe57thAnnualMeetingoftheAssociation forComputationalLinguistics,pp.3558–3567,Florence,Italy,July2019.AssociationforCom- putationalLinguistics. doi: 10.18653/v1/P19-1346. URLhttps://aclanthology.org/ P19-1346. Charles Fernyhough. Vygotsky, luria, and the social brain. Self and social regulation: Social interactionandthedevelopmentofsocialunderstandingandexecutivefunctions,pp.56–79,2010. Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Mari- beth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sonˇa Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements, 2022. URL https://storage.googleapis.com/deepmind-media/DeepMind. com/Authors-Notes/sparrow/sparrow-final.pdf. EhsanHosseini-Asl,BryanMcCann,Chien-ShengWu,SemihYavuz,andRichardSocher. Asimple languagemodelfortask-orienteddialogue. AdvancesinNeuralInformationProcessingSystems, 33:20179–20191,2020. WenlongHuang,PieterAbbeel,DeepakPathak,andIgorMordatch. Languagemodelsaszero-shot planners: Extractingactionableknowledgeforembodiedagents. arXivpreprintarXiv:2201.07207, 2022a. WenlongHuang,FeiXia,TedXiao,HarrisChan,JackyLiang,PeteFlorence,AndyZeng,Jonathan Tompson,IgorMordatch,YevgenChebotar,etal. Innermonologue: Embodiedreasoningthrough planningwithlanguagemodels. arXivpreprintarXiv:2207.05608,2022b. SiddharthKaramcheti,MeghaSrivastava,PercyLiang,andDorsaSadigh. Lila: Language-informed latentactions. InCoRL,pp.1379–1390,2021. URLhttps://proceedings.mlr.press/ v164/karamcheti22a.html. TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa. Large languagemodelsarezero-shotreasoners. arXivpreprintarXiv:2205.11916,2022. Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet- augmentedlanguagemodelsthroughfew-shotpromptingforopen-domainquestionanswering. arXivpreprintarXiv:2203.05115,2022. PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal, HeinrichKüttler,MikeLewis,Wen-tauYih,TimRocktäschel,etal. Retrieval-augmentedgenera- tionforknowledge-intensivenlptasks. AdvancesinNeuralInformationProcessingSystems,33: 9459–9474,2020. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang,EkinAkyürek,AnimaAnandkumar,JacobAndreas,IgorMordatch,AntonioTorralba, andYukeZhu.', 'UnlikeALFWorld, Webshopcontainsahighvarietyofstructuredandunstructuredtexts(e.g.producttitles,descriptions, andoptionscrawledfromAmazon),andrequiresanagenttopurchaseaproductbasedonauser instruction (e.g.“I am looking for a nightstand with drawers. It should have a nickel finish, and pricedlowerthan$140”)throughwebinteractions(e.g.search“nightstanddrawers”,choosebuttons suchas“color: modern-nickel-white”or“backtosearch”). Thistaskisevaluatedbyaveragescore (percentageofdesiredattributescoveredbythechosenproductaveragedacrossallepisodes)and successrate(percentageofepisodeswherethechosenproductsatisfiesallrequirements)on500test instructions. WeformulateActpromptswithactionstosearch, chooseproduct, chooseoptions, andbuy,withReActpromptsadditionallyreasoningtodeterminewhattoexplore,whentobuy, andwhatproductsoptionsarerelevanttotheinstruction. SeeTable6foranexampleprompt,and Table10formodelpredictionsintheAppendix. Wecomparetoanimitationlearning(IL)method 5Micheli&Fleuret(2021)finetunedaGPT-2modelon3553taskinstancesandachievedamuchimproved performancethanBUTLER,butitistrainedonalltasktypes,thusnotincludedasabaseline. 7 PublishedasaconferencepaperatICLR2023 Method Pick Clean Heat Cool Look Pick2 All Method Score SR Act (bestof6) 88 42 74 67 72 41 45 Act 62.3 30.1 ReAct (avg) 65 39 83 76 55 24 57 ReAct 66.6 40.0 ReAct (bestof6) 92 58 96 86 78 41 71 IL 59.9 29.1 ReAct-IM (avg) 55 59 60 55 23 24 48 IL+RL 62.4 28.7 ReAct-IM (bestof6) 62 68 87 57 39 33 53 Human 82.1 59.6 BUTLER g(bestof8) 33 26 70 76 17 12 22 Expert BUTLER(bestof8) 46 39 74 100 22 24 37 Table4: Scoreandsuc- Table 3: AlfWorld task-specific success rates (%). BUTLER and cessrate(SR)onWeb- BUTLER results are fromTable 4of Shridhar et al. (2020b). All shop. IL/IL+RLtaken g methodsusegreedydecoding,exceptthatBUTLERusesbeamsearch. fromYaoetal.(2022). trainedwith1,012humanannotatedtrajectories,andaimitation+reinforcementlearning(IL+RL) methodadditionallytrainedwith10,587traininginstructions. Results ReAct outperforms Act on both ALFWorld (Table 3) and Webshop (Table 4). On ALFWorld,thebestReActtrialachievesanaveragesuccessrateof71%,significantlyoutperforming thebestAct(45%)andBUTLER(37%)trials. Infact,eventheworseReActtrial(48%)beats thebesttrialofbothmethods. Moreover, theadvantageofReActoverActisconsistentacross sixcontrolledtrials,withrelativeperformancegainrangingfrom33%to90%andaveraging62%. Qualitatively, we saw that, without any thoughts at all, Act fails to correctly decompose goals intosmallersubgoals,orlosestrackofthecurrentstateoftheenvironment. Exampletrajectories comparingReActandActcanbefoundinAppendixD.2.1andAppendixD.2.2. OnWebshop,one-shotActpromptingalreadyperformsonparwithILandIL+RLmethods. With additionalsparsereasoning,ReActachievessignificantlybetterperformance,withanabsolute10% improvementoverthepreviousbestsuccessrate. Bycheckingexamples,wefindthatReActismore likelytoidentifyinstruction-relevantproductsandoptionsbyreasoningtobridgethegapbetween noisyobservationsandactions(e.g.“For‘space-savingottomanbenchforlivingroom’,theitem hasoptions‘39x18x18inch’and‘blue’andseemsgoodtobuy.”). However,existingmethodsare stillfarfromtheperformanceofexperthumans(Table4),whoperformsignificantlymoreproduct explorationsandqueryre-formulationsthatarestillchallengingforprompting-basedmethods. Onthevalueofinternalreasoningvs. externalfeedback Toourknowledge,ReActisthefirst demonstrationofcombinedreasoningandactionusinganLLMappliedtoaninteractiveenvironment withinaclosed-loopsystem. PerhapstheclosestpriorworkisInnerMonologue(IM),fromHuang et al. (2022b), in which actions from an embodied agent are motivated by an eponymous “inner monologue”. However,IM’s“innermonologue”islimitedtoobservationsoftheenvironment stateandwhatneedstobecompletedbytheagentforthegoaltobesatisfied. Incontrast,the reasoningtracesinReActfordecisionmakingisflexibleandsparse,allowingdiversereasoning types(seeSection2)tobeinducedfordifferenttasks. TodemonstratethedifferencesbetweenReActandIM,andtohighlighttheimportanceofinternal reasoningvs. simplereactionstoexternalfeedback,werananablationexperimentusingathought patterncomposedofIM-likedenseexternalfeedback.AscanbeseeninTable3,ReActsubstantially outperforms IM-style prompting (ReAct-IM) (71 vs.53 overall success rate), with consistent advantagesonfiveoutofsixtasks. Qualitatively,weobservedthatReAct-IMoftenmademistakes inidentifyingwhensubgoalswerefinished,orwhatthenextsubgoalshouldbe,duetoalackofhigh- levelgoaldecomposition. Additionally,manyReAct-IMtrajectoriesstruggledtodeterminewhere anitemwouldlikelybewithintheALFWorldenvironment,duetoalackofcommonsensereasoning. BothshortcomingscanbeaddressedintheReActparadigm. MoredetailsaboutReAct-IMisin AppendixB.2. AnexamplepromptforReAct-IMcanbefoundinAppendixC.4,andanexample trajectoryinAppendixD.2.3. 8 PublishedasaconferencepaperatICLR2023 5 RELATED WORK Languagemodelforreasoning Perhapsthemostwell-knownworkofusingLLMsforreasoning isChain-of-Thought(CoT)(Weietal.,2022),whichrevealstheabilityofLLMstoformulatetheir own“thinkingprocedure”forproblemsolving. Severalfollow-upworkshavesincebeenperformed, including least-to-most prompting for solving complicated tasks (Zhou et al., 2022), zero-shot- CoT (Kojima et al., 2022), and reasoning with self-consistency (Wang et al., 2022a). Recently, (Madaan&Yazdanbakhsh,2022)systematicallystudiedtheformulationandstructureofCoT,and observedthatthepresenceofsymbols,patternsandtextsiscrucialtotheeffectivenessofCoT.Other workhasalsobeenextendedtomoresophisticatedreasoningarchitecturebeyondsimpleprompting. ForexampleSelection-Inference(Creswelletal.,2022)dividesthereasoningprocessintotwosteps of“selection”and“inference”. STaR(Zelikmanetal.,2022)bootstrapsthereasoningprocessby finetuningthemodeloncorrectrationalesgeneratedbythemodelitself. Faithfulreasoning(Creswell &Shanahan,2022)decomposesmulti-stepreasoningintothreesteps,eachperformedbyadedicated LMrespectively. SimilarapproacheslikeScratchpad(Nyeetal.,2021),whichfinetunesaLMon intermediatecomputationsteps,alsodemonstrateimprovementonmulti-stepcomputationproblems. Incontrasttothesemethods,ReActperformsmorethanjustisolated,fixedreasoning,andintegrates modelactionsandtheircorrespondingobservationsintoacoherentstreamofinputsforthemodelto reasonmoreaccuratelyandtackletasksbeyondreasoning(e.g.interactivedecisionmaking). Languagemodelfordecisionmaking ThestrongcapabilityofLLMshasenabledthemtoperform tasksbeyondlanguagegeneration,anditisbecomingmorepopulartotakeadvantageofLLMsasa policymodelfordecisionmaking,especiallyininteractiveenvironments. WebGPT(Nakanoetal., 2021)usesanLMtointeractwithwebbrowsers,navigatethroughwebpages,andinferanswersto complicatedquestionsfromELI5(Fanetal.,2019). IncomparisontoReAct,WebGPTdoesnot explicitlymodelthethinkingandreasoningprocedure,insteadrelyonexpensivehumanfeedbackfor reinforcementlearning. Inconversationmodeling,chatbotslikeBlenderBot(Shusteretal.,2022b) andSparrow(Glaeseetal.,2022)andtask-orienteddialoguesystemslikeSimpleTOD(Hosseini-Asl etal.,2020)alsotrainLMstomakedecisionaboutAPIcalls. UnlikeReAct,theydonotexplicitly considerthereasoningprocedureeither,andalsoreliesonexpensivedatasetsandhumanfeedback collectionsforpolicylearning. Incontrast,ReActlearnsapolicyinamuchcheaperway,sincethe decisionmakingprocessonlyrequireslanguagedescriptionofthereasoningprocedure.6 LLMShavealsobeenincreasinglyemployedininteractiveandembodiedenvironmentsforplanning anddecisionmaking. PerhapsmostrelevanttoReActinthisrespectareSayCan(Ahnetal.,2022) andInnerMonologue(Huangetal.,2022b),whichuseLLMsforroboticactionplanninganddecision making. InSayCan,LLMswerepromptedtodirectlypredictpossibleactionsarobotcantake,which isthenrerankedbyanaffordancemodelgroundedonthevisualenvironmentsforfinalprediction. InnerMonologuemadefurtherimprovementsbyaddingtheeponymous“innermonologue",whichis implementedasinjectedfeedbackfromtheenvironment. Toourknowledge,InnerMonologueisthe firstworkthatdemonstratessuchaclosed-loopsystem,whichReActbuildson. However,weargue', 'is more accurate in formulating reasoning structure but can easily suffer from hallucinated facts orthoughts. WethereforeproposetoincorporateReActandCoT-SC,andletthemodeldecide whentoswitchtotheothermethodbasedonthefollowingheuristics: A)ReAct→CoT-SC:when ReActfailstoreturnananswerwithingivensteps,backofftoCoT-SC.Weset7and5stepsfor HotpotQAandFEVERrespectivelyaswefindmorestepswillnotimproveReActperformance3. B) CoT-SC→ReAct: whenthemajorityansweramongnCoT-SCsamplesoccurslessthann/2 times(i.e.internalknowledgemightnotsupportthetaskconfidently),backofftoReAct. Finetuning Due to the challenge of manually annotating reasoning traces and actions at scale, we consider a bootstraping approach similar to Zelikman et al. (2022), using 3,000 trajectories withcorrectanswersgeneratedbyReAct(alsoforotherbaselines)tofinetunesmallerlanguage models(PaLM-8/62B)todecodetrajectories(allthoughts, actions, observations)conditionedon inputquestions/claims. MoredetailsareinAppendixB.1. 3.3 RESULTSANDOBSERVATIONS ReActoutperformsActconsistently Table1showsHotpotQAandFeverresultsusingPaLM- 540Basthebasemodelwithdifferentpromptingmethods. WenotethatReActisbetterthanAct onbothtasks,demonstratingthevalueofreasoningtoguideacting,especiallyforsynthesizingthe finalanswer,asshowninFigure1(1c-d). Fine-tuningresults3alsoconfirmthebenefitofreasoning tracesformoreinformedacting. 3Ofalltrajectorieswithcorrectfinalanswers,thosewith7stepsonHotpotQAand5stepsonFEVERonly takeup0.84%and1.33%respectively. 5 PublishedasaconferencepaperatICLR2023 Type Definition ReAct CoT Truepositive Correctreasoningtraceandfacts 94% 86% Success Falsepositive Hallucinatedreasoningtraceorfacts 6% 14% Reasoningerror Wrongreasoningtrace(includingfailingtorecoverfromrepetitivesteps) 47% 16% Searchresulterror Searchreturnemptyordoesnotcontainusefulinformation 23% - Failure Hallucination Hallucinatedreasoningtraceorfacts 0% 56% Labelambiguity Rightpredictionbutdidnotmatchthelabelprecisely 29% 28% Table 2: Types of success and failure modes of ReAct and CoT on HotpotQA, as well as their percentagesinrandomlyselectedexamplesstudiedbyhuman. ReActvs. CoT Ontheotherhand,ReActoutperformsCoTonFever(60.9vs.56.3)andslightly lagsbehindCoTonHotpotQA(27.4vs.29.4). FeverclaimsforSUPPORTS/REFUTESmightonly differbyaslightamount(seeAppendixD.1),soactingtoretrieveaccurateandup-to-dateknowledge isvital. TobetterunderstandthebehavioraldifferencebetweenReActandCoTonHotpotQA,we randomlysampled50trajectorieswithcorrectandincorrectanswers(judgedbyEM)fromReAct andCoTrespectively(thus200examplesintotal),andmanuallylabeledtheirsuccessandfailure modesinTable2. Somekeyobservationsareasfollows: A)HallucinationisaseriousproblemforCoT,resultinginmuchhigherfalsepositiveratethan ReAct(14%vs. 6%)insuccessmode,andmakeupitsmajorfailuremode(56%). Incontrast,the problemsolvingtrajectoryofReActismoregrounded,fact-driven,andtrustworthy,thankstothe accessofanexternalknowledgebase. B)Whileinterleavingreasoning,actionandobservationstepsimprovesReAct’sgrounded- nessandtrustworthiness,suchastructuralconstraintalsoreducesitsflexibilityinformulating reasoningsteps,leadingtomorereasoningerrorratethanCoT.wenotethatthereisonefrequent errorpatternspecifictoReAct,inwhichthemodelrepetitivelygeneratesthepreviousthoughtsand actions,andwecategorizeitaspartof“reasoningerror”asthemodelfailstoreasonaboutwhatthe propernextactiontotakeandjumpoutoftheloop4. C) For ReAct, successfully retrieving informative knowledge via search is critical. Non- informativesearch,whichcountsfor23%oftheerrorcases,derailsthemodelreasoningandgives itahardtimetorecoverandreformulatethoughts. Thisisperhapsanexpectedtrade-offbetween factualityandflexibility,whichmotivatesourproposedstrategiesofcombiningtwomethods. We provide examples for each success and failure modes in Appendix E.1. We also find some HotpotQAquestionsmaycontainoutdatedanswerlabels,seeFigure4forexample. ReAct+CoT-SCperformbestforpromptingLLMs AlsoshowninTable1,thebestprompting method on HotpotQA and Fever are ReAct → CoT-SC and CoT-SC → ReAct respectively. Furthermore,Figure2showshowdifferentmethodsperformwithrespecttothenumberofCoT-SC samplesused. WhiletwoReAct+CoT-SCmethodsareadvantageousatonetaskeach,theyboth significantly and consistently outperform CoT-SC across different number of samples, reaching CoT-SCperformancewith21samplesusingmerely3-5samples. Theseresultsindicatethevalueof properlycombiningmodelinternalknowledgeandexternalknowledgeforreasoningtasks. ReActperformsbestforfine-tuning Figure3showsthescalingeffectofprompting/finetuning fourmethods(Standard,CoT,Act,ReAct)onHotpotQA.WithPaLM-8/62B,promptingReAct performsworstamongfourmethodsduetothedifficultytolearnbothreasoningandactingfrom in-contextexamples. However,whenfinetunedwithjust3,000examples,ReActbecomesthebest methodamongthefour,withPaLM-8BfinetunedReActoutperformingallPaLM-62Bprompting methods,andPaLM-62BfinetunedReActoutperformingall540Bpromptingmethods. Incontrast, finetuningStandardorCoTissignificantlyworsethanfinetuningReActorActforbothPaLM- 8/62B,astheformeressentiallyteachesmodelstomemorize(potentiallyhalluincated)knowledge facts,andthelatterteachesmodelshowto(reasonand)acttoaccessinformationfromWikipedia,a moregeneralizableskillforknowledgereasoning. Asallpromptingmethodsarestillsignificantly far from domain-specific state-of-the-art approaches (Table 1), we believe finetuning with more human-writtendatamightbeabetterwaytounleashthepowerofReAct. 4Wesuspectthatthiscouldbeduetothesub-optimalgreedydecodingprocedure,andfutureworkusing betterdecoding(e.g.beamsearch)mighthelpaddressthisissue. 6 PublishedasaconferencepaperatICLR2023 30 25 20 15 10 5 0 8b 62b 540b size ME AQtoptoH learning = prompt learning = finetune Method Standard CoT Act ReAct 8b 62b 540b size Figure3:ScalingresultsforpromptingandfinetuningonHotPotQAwithReAct(ours)andbaselines. 4 DECISION MAKING TASKS We also test ReAct on two language-based interactive decision-making tasks, ALFWorld and WebShop,bothofwhichfeaturecomplexenvironmentsthatrequireagentstoactoverlonghorizons withsparserewards,warrantingtheneedforreasoningtoactandexploreeffectively. ALFWorld ALFWorld(Shridharetal.,2020b)(Figure1(2))isasynthetictext-basedgamedesigned to align with the embodied ALFRED benchmark (Shridhar et al., 2020a). It includes 6 types of tasksinwhichanagentneedstoachieveahigh-levelgoal(e.g.examinepaperunderdesklamp)by navigatingandinteractingwithasimulatedhouseholdviatextactions(e.g. gotocoffeetable1,take paper2,usedesklamp1). Ataskinstancecanhavemorethan50locationsandtakeanexpertpolicy morethan50stepstosolve,thuschallenginganagenttoplanandtracksubgoals,aswellasexplore systematically(e.g.checkalldesksonebyonefordesklamp). Inparticular,onechallengebuiltinto ALFWorldistheneedtodeterminelikelylocationsforcommonhouseholditems(e.g.desklampswill likelybeondesks,shelfs,ordressers),makingthisenvironmentagoodfitforLLMstoexploittheir pretrainedcommonsenseknowledge. TopromptReAct,werandomlyannotatethreetrajectories from the training set for each task type, where each trajectory includes sparse thoughts that (1) decomposethegoal,(2)tracksubgoalcompletion,(3)determinethenextsubgoal,and(4)reasonvia commonsensewheretofindanobjectandwhattodowithit. WeshowpromptsusedforALFWorld inAppendixC.4. Following Shridharetal.(2020b),weevaluateon134unseenevaluationgames in a task-specific setup. For robustness, we construct 6 prompts for each task type through each permutationof2annotatedtrajectoriesfromthe3weannotate. Actpromptsareconstructedusing thesametrajectories,butwithoutthoughts—sincetaskinstancesarerandomlychosenfromthe trainingset,itfavorsneitherReActnorActandprovidesafairandcontrolledcomparisontotestthe importanceofsparsethoughts. Forbaselines,weuseBUTLER(Shridharetal.,2020b),animitation learningagenttrainedon105experttrajectoriesforeachtasktype5. WebShop Can ReAct also interact with noisy real-world language environments for practical applications? We investigate WebShop (Yao et al., 2022), a recently proposed online shopping websiteenvironmentwith1.18Mreal-worldproductsand12khumaninstructions.']], 'uris': None, 'data': None}